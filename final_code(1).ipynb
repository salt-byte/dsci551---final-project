{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f028947",
   "metadata": {},
   "source": [
    "# JSON tokenizer & JSONL parser "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346daa32",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8090a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Token:\n",
    "    #Represents a JSON token with type, value, and position.\n",
    "    def __init__(self, t, v, pos):\n",
    "        self.type, self.value, self.pos = t, v, pos\n",
    "\n",
    "class Tokenizer:\n",
    "    ws = set(\" \\t\\r\\n\")\n",
    "    def __init__(self, text):\n",
    "        self.text, self.n, self.i = text, len(text), 0\n",
    "\n",
    "    def peek(self):\n",
    "        return self.text[self.i] if self.i < self.n else ''\n",
    "\n",
    "    def next(self):\n",
    "        ch = self.peek()\n",
    "        self.i += 1\n",
    "        return ch\n",
    "\n",
    "    def skip_ws(self):\n",
    "        while self.i < self.n and self.text[self.i] in self.ws:\n",
    "            self.i += 1\n",
    "\n",
    "    def read_str(self, start):\n",
    "        self.next()  # skip opening quote\n",
    "        out = []\n",
    "        while True:\n",
    "            if self.i >= self.n:\n",
    "                raise SyntaxError(f\"String not closed (from {start})\")\n",
    "            ch = self.next()\n",
    "            if ch == '\"':\n",
    "                break\n",
    "            if ch == '\\\\':\n",
    "                if self.i >= self.n:\n",
    "                    raise SyntaxError(\"Bad escape sequence\")\n",
    "                esc = self.next()\n",
    "                m = {'\"':'\"', '\\\\':'\\\\', '/':'/', 'b':'\\b', 'f':'\\f', 'n':'\\n', 'r':'\\r', 't':'\\t'}\n",
    "                if esc in m:\n",
    "                    out.append(m[esc])\n",
    "                else:\n",
    "                    raise SyntaxError(f\"Unknown escape \\\\{esc}\")\n",
    "            else:\n",
    "                out.append(ch)\n",
    "        return ''.join(out)\n",
    "\n",
    "    def read_num(self, start):\n",
    "        j = self.i\n",
    "        if self.peek() == '-':\n",
    "            self.next()\n",
    "        if self.peek() == '0':\n",
    "            self.next()\n",
    "        else:\n",
    "            if not self.peek().isdigit():\n",
    "                raise SyntaxError(f\"Bad number at {start}\")\n",
    "            while self.peek().isdigit():\n",
    "                self.next()\n",
    "        if self.peek() == '.':\n",
    "            self.next()\n",
    "            if not self.peek().isdigit():\n",
    "                raise SyntaxError(\"Bad decimal\")\n",
    "            while self.peek().isdigit():\n",
    "                self.next()\n",
    "        s = self.text[j:self.i]\n",
    "        return float(s) if '.' in s else int(s)\n",
    "\n",
    "    def read_kw(self, start): #read keyword:true, false,null \n",
    "        for k, v in [(\"true\", True), (\"false\", False), (\"null\", None)]:\n",
    "            if self.text.startswith(k, self.i):\n",
    "                self.i += len(k)\n",
    "                return v\n",
    "        raise SyntaxError(f\"Unknown literal near {start}\")\n",
    "\n",
    "    def tokens(self):\n",
    "        while True:\n",
    "            self.skip_ws()\n",
    "            if self.i >= self.n:\n",
    "                yield Token(\"EOF\", None, self.i) #end of file\n",
    "                return\n",
    "            ch = self.peek()\n",
    "            pos = self.i\n",
    "            if ch in '{}[]:,':\n",
    "                self.next()\n",
    "                yield Token(ch, ch, pos)\n",
    "            elif ch == '\"':\n",
    "                yield Token(\"STR\", self.read_str(pos), pos)#string\n",
    "            elif ch in '-0123456789':\n",
    "                yield Token(\"NUM\", self.read_num(pos), pos)#number\n",
    "            else:\n",
    "                yield Token(\"KW\", self.read_kw(pos), pos)#keyword\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83594aa5",
   "metadata": {},
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0005b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#focus on the relationship between tokens\n",
    "class Stream:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.gen, self.buf = tokenizer.tokens(), []\n",
    "\n",
    "    def peek(self):\n",
    "        if not self.buf:\n",
    "            self.buf.append(next(self.gen))\n",
    "        return self.buf[0]\n",
    "\n",
    "    def next(self):\n",
    "        if self.buf:\n",
    "            return self.buf.pop(0)\n",
    "        return next(self.gen)\n",
    "\n",
    "    def expect(self, t):\n",
    "        tok = self.next()\n",
    "        if tok.type != t:\n",
    "            raise SyntaxError(f\"Expect {t} at {tok.pos}, got {tok.type}\")\n",
    "        return tok\n",
    "\n",
    "class Parser:\n",
    "    def parse(self, text):\n",
    "        ts = Stream(Tokenizer(text))\n",
    "        val = self.value(ts)\n",
    "        if ts.peek().type != \"EOF\":\n",
    "            raise SyntaxError(\"Extra content after JSON\")\n",
    "        return val\n",
    "\n",
    "    def value(self, ts):\n",
    "        t = ts.peek()\n",
    "        if t.type == '{': return self.obj(ts)\n",
    "        if t.type == '[': return self.arr(ts)\n",
    "        if t.type == 'STR': return ts.next().value\n",
    "        if t.type == 'NUM': return ts.next().value\n",
    "        if t.type == 'KW':  return ts.next().value\n",
    "        raise SyntaxError(f\"Unexpected token {t.type}\")\n",
    "\n",
    "    def obj(self, ts):\n",
    "        ts.expect('{')\n",
    "        o = {}\n",
    "        if ts.peek().type == '}':\n",
    "            ts.next()\n",
    "            return o\n",
    "        while True:\n",
    "            k = ts.expect(\"STR\").value\n",
    "            ts.expect(':')\n",
    "            o[k] = self.value(ts)\n",
    "            t = ts.peek()\n",
    "            if t.type == ',':\n",
    "                ts.next()\n",
    "            elif t.type == '}':\n",
    "                ts.next(); break\n",
    "            else:\n",
    "                raise SyntaxError(f\"Unexpected {t.type}\")\n",
    "        return o\n",
    "\n",
    "    def arr(self, ts):\n",
    "        ts.expect('[')\n",
    "        a = []\n",
    "        if ts.peek().type == ']':\n",
    "            ts.next()\n",
    "            return a\n",
    "        while True:\n",
    "            a.append(self.value(ts))\n",
    "            t = ts.peek()\n",
    "            if t.type == ',':\n",
    "                ts.next()\n",
    "            elif t.type == ']':\n",
    "                ts.next(); break\n",
    "            else:\n",
    "                raise SyntaxError(f\"Unexpected {t.type}\")\n",
    "        return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b896b69",
   "metadata": {},
   "source": [
    "## Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43ef8a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collection:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data if isinstance(data, list) else [data]#ensure that data is a list or turned to be a list\n",
    "    \n",
    "    def _extract_key(self, doc, key):\n",
    "        \"\"\"supports dot notation\"\"\"\n",
    "        ks = key.split(\".\")\n",
    "        cur = doc\n",
    "        for k in ks:\n",
    "            if not isinstance(cur, dict) or k not in cur:\n",
    "                return None\n",
    "            cur = cur[k]\n",
    "        return cur\n",
    "\n",
    "    def find(self, query=None):#query like{\"attitude_count\":500,\"likes\":2}\n",
    "        \n",
    "        if query is None:#find all data\n",
    "            return self.data\n",
    "\n",
    "        def match(doc, query): #to check if doc fits query\n",
    "            for key, value in query.items():\n",
    "                cur = self._extract_key(doc, key)  \n",
    "                if cur != value:\n",
    "                    return False\n",
    "            return True\n",
    "\n",
    "        return [doc for doc in self.data if match(doc, query)]\n",
    "\n",
    "    def project(self, fields):\n",
    "\n",
    "    #Return documents with only selected fields.\n",
    "    #Example: fields = [\"user\", \"text\"]\n",
    "\n",
    "        result = []\n",
    "        for doc in self.data:\n",
    "            projected = {}\n",
    "            for field in fields:\n",
    "                #use _extract_key to process nested key\n",
    "                projected[field] = self._extract_key(doc, field)\n",
    "            result.append(projected)\n",
    "        return result\n",
    "\n",
    "    def groupby(self, key):\n",
    "        groups = {}\n",
    "        for doc in self.data:\n",
    "            group_value = self._extract_key(doc, key) \n",
    "            groups.setdefault(group_value, []).append(doc)\n",
    "        return groups\n",
    "\n",
    "    def aggregate(self, group_key, agg_func):\n",
    "        #Apply an aggregation function (sum, count, avg, etc.) on each group.\n",
    "        grouped = self.groupby(group_key)\n",
    "        result = {}\n",
    "        for k, docs in grouped.items():\n",
    "            result[k] = agg_func(docs)\n",
    "        return result\n",
    "\n",
    "\n",
    "    def hash_join(self, other, key_self, key_other, join_type=\"inner\"):\n",
    "        \"\"\"\n",
    "        join_type: inner / left / right / full\n",
    "        \"\"\"\n",
    "\n",
    "        # Build hash map for other\n",
    "        hashmap = {}\n",
    "        for doc in other.data:\n",
    "            val = self._extract_key(doc, key_other)\n",
    "            hashmap.setdefault(val, []).append(doc)\n",
    "\n",
    "        result = []\n",
    "        # Process left side (self)\n",
    "        matched_right_keys = set()\n",
    "\n",
    "        for doc_left in self.data:\n",
    "            val_left = self._extract_key(doc_left, key_self)\n",
    "            if val_left in hashmap:\n",
    "                for doc_right in hashmap[val_left]:\n",
    "                    matched_right_keys.add(id(doc_right))\n",
    "                    result.append({\n",
    "                        \"left\": doc_left,\n",
    "                        \"right\": doc_right\n",
    "                    })\n",
    "            else:\n",
    "                if join_type in (\"left\", \"full\"):\n",
    "                    result.append({\n",
    "                        \"left\": doc_left,\n",
    "                        \"right\": None\n",
    "                    })\n",
    "\n",
    "        # Process unmatched right side (right join or full join)\n",
    "        if join_type in (\"right\", \"full\"):\n",
    "            for doc_right in other.data:\n",
    "                if id(doc_right) not in matched_right_keys:\n",
    "                    result.append({\n",
    "                        \"left\": None,\n",
    "                        \"right\": doc_right\n",
    "                    })\n",
    "\n",
    "        return result\n",
    "    def pipeline(self, query=None, project_fields=None,\n",
    "                 group_key=None, agg_func=None,\n",
    "                 join_collection=None, join_self_key=None, \n",
    "                 join_other_key=None, join_type=\"inner\"):\n",
    "        data = self.data\n",
    "\n",
    "        if query:\n",
    "            data = Collection(data).find(query)\n",
    "\n",
    "        if project_fields:\n",
    "            data = Collection(data).project(project_fields)\n",
    "\n",
    "        if group_key and agg_func:\n",
    "            data = Collection(data).aggregate(group_key, agg_func)\n",
    "\n",
    "        if join_collection:\n",
    "            data = Collection(data).hash_join(\n",
    "                join_collection,\n",
    "                join_self_key,\n",
    "                join_other_key,\n",
    "                join_type\n",
    "            )\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b932c962",
   "metadata": {},
   "source": [
    "## aggregate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52d84c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def agg_count(field=None):\n",
    "    return lambda docs: len(docs)\n",
    "\n",
    "def agg_sum(field):\n",
    "    return lambda docs: sum(\n",
    "        doc.get(field, 0) for doc in docs\n",
    "        if isinstance(doc.get(field), (int, float))\n",
    "    )\n",
    "\n",
    "def agg_max(field):\n",
    "    return lambda docs: max(\n",
    "        doc.get(field) for doc in docs\n",
    "        if isinstance(doc.get(field), (int, float))\n",
    "    )\n",
    "\n",
    "def agg_min(field):\n",
    "    return lambda docs: min(\n",
    "        doc.get(field) for doc in docs\n",
    "        if isinstance(doc.get(field), (int, float))\n",
    "    )\n",
    "\n",
    "def agg_avg(field):\n",
    "    return lambda docs: (\n",
    "        sum(doc.get(field, 0) for doc in docs\n",
    "            if isinstance(doc.get(field), (int, float)))\n",
    "        / len(docs)\n",
    "        if docs else None\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5def3ff5",
   "metadata": {},
   "source": [
    "## chunk processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "494e502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_chunks(path, chunk_size=5000):\n",
    "    \"\"\"\n",
    "    Generic loader:\n",
    "        - if JSONL:  one JSON object per line\n",
    "        - if JSON array: [ {...}, {...} ]\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        first_char = f.read(1)#to identify whether it is a josn or a jsonl\n",
    "        f.seek(0)#go back to the begining \n",
    "\n",
    "        if first_char == \"[\":  # JSON array\n",
    "            text = f.read()\n",
    "            parser = Parser()\n",
    "            arr = parser.parse(text)\n",
    "            for i in range(0, len(arr), chunk_size):\n",
    "                yield arr[i:i + chunk_size]\n",
    "        else: # JSONL\n",
    "            parser = Parser()\n",
    "            buffer = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                buffer.append(parser.parse(line))\n",
    "                if len(buffer) >= chunk_size:\n",
    "                    yield buffer\n",
    "                    buffer = []\n",
    "            if buffer:\n",
    "                yield buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1a4c6",
   "metadata": {},
   "source": [
    "## merge partial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "834ed614",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialAgg:\n",
    "    \"\"\"merge of partial aggregation results\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_count(v1, v2):\n",
    "        return v1 + v2\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_sum(v1, v2):\n",
    "        return v1 + v2\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_max(v1, v2):\n",
    "        return max(v1, v2)\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_min(v1, v2):\n",
    "        return min(v1, v2)\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_avg(avg1, count1, avg2, count2):\n",
    "        # weighted average\n",
    "        total = count1 + count2\n",
    "        return (avg1 * count1 + avg2 * count2) / total, total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6aba6c",
   "metadata": {},
   "source": [
    "## chunk processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2d1685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_engagement_by_location(filepath, chunk_size=5000):\n",
    "    \"\"\"\n",
    "    Calculates the Average Engagement Rate (AER) grouped by IP location \n",
    "    for large datasets using chunked processing and partial aggregation merging.\n",
    "    This demonstrates the project's scaling requirement.\n",
    "    AER = (Total Reposts + Total Comments + Total Attitudes) / Total Posts\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. initialize four global partial result containers\n",
    "    # Dictionaries to store merged partial aggregation results globally\n",
    "    partial_counts = {}\n",
    "    partial_reposts_sums = {}\n",
    "    partial_comments_sums = {}\n",
    "    partial_attitudes_sums = {}\n",
    "    \n",
    "    # 2. process the file chunk by chunk\n",
    "    for chunk in load_json_chunks(filepath, chunk_size):\n",
    "        coll = Collection(chunk)\n",
    "    \n",
    "        # local aggregation calculations (Grouped by \"ip_location\")\n",
    "        chunk_counts = coll.aggregate(\"ip_location\", agg_count())\n",
    "        chunk_reposts = coll.aggregate(\"ip_location\", agg_sum(\"reposts_count\"))\n",
    "        chunk_comments = coll.aggregate(\"ip_location\", agg_sum(\"comments_count\"))\n",
    "        chunk_attitudes = coll.aggregate(\"ip_location\", agg_sum(\"attitudes_count\"))\n",
    "        \n",
    "        # 3. merge Local Results\n",
    "        \n",
    "        # Merge Counts (Total Posts)\n",
    "        for loc, count in chunk_counts.items():\n",
    "            current_count = partial_counts.get(loc, 0)\n",
    "            # use PartialAgg.merge_count to combine current global total with local chunk total\n",
    "            partial_counts[loc] = PartialAgg.merge_count(current_count, count)\n",
    "            \n",
    "        # Merge Reposts Sums\n",
    "        for loc, total in chunk_reposts.items():\n",
    "            current_total = partial_reposts_sums.get(loc, 0)\n",
    "            # Use PartialAgg.merge_sum for addition\n",
    "            partial_reposts_sums[loc] = PartialAgg.merge_sum(current_total, total)\n",
    "            \n",
    "        # Merge Comments Sums\n",
    "        for loc, total in chunk_comments.items():\n",
    "            current_total = partial_comments_sums.get(loc, 0)\n",
    "            partial_comments_sums[loc] = PartialAgg.merge_sum(current_total, total)\n",
    "            \n",
    "        # Merge Attitudes (Likes) Sums\n",
    "        for loc, total in chunk_attitudes.items():\n",
    "            current_total = partial_attitudes_sums.get(loc, 0)\n",
    "            partial_attitudes_sums[loc] = PartialAgg.merge_sum(current_total, total)\n",
    "            \n",
    "    # 4. Calculate Final Average Engagement Rate (Final Calculation)\n",
    "    final_results = {}\n",
    "    for loc in partial_counts:\n",
    "        # Get all global sums\n",
    "        total_interactions = (\n",
    "            partial_reposts_sums.get(loc, 0) +\n",
    "            partial_comments_sums.get(loc, 0) +\n",
    "            partial_attitudes_sums.get(loc, 0)\n",
    "        )\n",
    "        total_posts = partial_counts[loc]\n",
    "        \n",
    "        # Calculate Average Engagement Rate, prevent division by zero\n",
    "        avg_engagement_rate = total_interactions / total_posts if total_posts else 0\n",
    "        \n",
    "        final_results[loc] = {\n",
    "            \"Total_Posts\": total_posts,\n",
    "            \"Avg_Engagement_Rate\": avg_engagement_rate\n",
    "        }\n",
    "    \n",
    "    return final_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
