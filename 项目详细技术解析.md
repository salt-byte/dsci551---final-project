# 项目详细技术解析

## 目录
1. [JSON Tokenizer（词法分析器）](#1-json-tokenizer词法分析器)
2. [JSON Parser（语法分析器）](#2-json-parser语法分析器)
3. [Collection 类（数据集合操作）](#3-collection-类数据集合操作)
4. [聚合函数实现](#4-聚合函数实现)
5. [分块处理机制](#5-分块处理机制)
6. [部分聚合合并](#6-部分聚合合并)
7. [实际应用案例](#7-实际应用案例)

---

## 1. JSON Tokenizer（词法分析器）

### 1.1 Token 类

```python
class Token:
    def __init__(self, t, v, pos):
        self.type, self.value, self.pos = t, v, pos
```

**作用**：表示 JSON 中的一个词法单元（Token）

**参数说明**：
- `t` (type): Token 的类型，如 "STR"（字符串）、"NUM"（数字）、"{"（对象开始）等
- `v` (value): Token 的实际值，如字符串内容、数字值等
- `pos` (position): Token 在源文本中的位置，用于错误定位

**示例**：
- `Token("STR", "hello", 10)` 表示位置 10 处有一个值为 "hello" 的字符串
- `Token("{", "{", 0)` 表示位置 0 处有一个左花括号

---

### 1.2 Tokenizer 类初始化

```python
class Tokenizer:
    ws = set(" \t\r\n")  # 空白字符集合
    def __init__(self, text):
        self.text, self.n, self.i = text, len(text), 0
```

**作用**：初始化词法分析器

**关键变量**：
- `ws`: 类变量，定义所有空白字符（空格、制表符、回车、换行）
- `text`: 要解析的 JSON 文本字符串
- `n`: 文本总长度
- `i`: 当前读取位置（索引），从 0 开始

**设计思路**：使用单个指针 `i` 从左到右扫描文本，逐步识别 Token

---

### 1.3 基础方法

#### peek() - 预览字符

```python
def peek(self):
    return self.text[self.i] if self.i < self.n else ''
```

**作用**：查看当前位置的字符，但不移动指针

**返回值**：
- 如果还有字符：返回当前字符
- 如果到达末尾：返回空字符串

**为什么需要**：在决定如何处理字符之前，需要先"看看"下一个字符是什么，例如判断数字是否包含小数点

---

#### next() - 读取并移动

```python
def next(self):
    ch = self.peek()
    self.i += 1
    return ch
```

**作用**：读取当前字符，并将指针向前移动一位

**使用场景**：当确定要消耗这个字符时使用

---

#### skip_ws() - 跳过空白字符

```python
def skip_ws(self):
    while self.i < self.n and self.text[self.i] in self.ws:
        self.i += 1
```

**作用**：跳过所有连续的空白字符

**为什么重要**：JSON 中空白字符是无关紧要的，需要忽略它们才能正确识别 Token

**示例**：
- 输入：`{   "name"   :   "John"   }`
- 经过 skip_ws 后，可以正确识别出各个 Token，忽略空格

---

### 1.4 读取字符串（read_str）

```python
def read_str(self, start):
    self.next()  # skip opening quote（跳过开始的双引号）
    out = []
    while True:
        if self.i >= self.n:
            raise SyntaxError(f"String not closed (from {start})")
        ch = self.next()
        if ch == '"':
            break  # 遇到结束引号
        if ch == '\\':  # 遇到转义符
            if self.i >= self.n:
                raise SyntaxError("Bad escape sequence")
            esc = self.next()  # 读取转义后的字符
            m = {'"':'"', '\\':'\\', '/':'/', 'b':'\b', 'f':'\f', 'n':'\n', 'r':'\r', 't':'\t'}
            if esc in m:
                out.append(m[esc])
            else:
                raise SyntaxError(f"Unknown escape \\{esc}")
        else:
            out.append(ch)
    return ''.join(out)
```

**作用**：读取一个完整的 JSON 字符串值

**详细流程**：

1. **跳过开始引号**：`self.next()` 消耗 `"`

2. **循环读取字符**：
   - 检查是否到达文件末尾（字符串未闭合错误）
   - 读取下一个字符

3. **处理普通字符**：
   - 如果是结束引号 `"`，跳出循环
   - 如果是普通字符，加入结果列表

4. **处理转义序列**：
   - 遇到 `\` 时，读取下一个字符
   - 根据转义字符映射表转换为实际字符：
     - `\"` → `"`（引号）
     - `\\` → `\`（反斜杠）
     - `\n` → 换行符
     - `\t` → 制表符
     - `\b` → 退格符
     - `\f` → 换页符
     - `\r` → 回车符
     - `\/` → `/`
   - 如果是不认识的转义序列，抛出错误

5. **返回结果**：将字符列表连接成字符串

**示例**：
```python
# 输入：""hello\nworld""
# 过程：
# 1. 跳过第一个 "
# 2. 读取 h, e, l, l, o
# 3. 遇到 \n，转换为换行符
# 4. 读取 w, o, r, l, d
# 5. 遇到 "，结束
# 结果：'hello\nworld'
```

---

### 1.5 读取数字（read_num）

```python
def read_num(self, start):
    j = self.i  # 记录开始位置
    if self.peek() == '-':  # 负数
        self.next()
    if self.peek() == '0':  # 以0开头
        self.next()
    else:  # 其他数字
        if not self.peek().isdigit():
            raise SyntaxError(f"Bad number at {start}")
        while self.peek().isdigit():  # 读取整数部分
            self.next()
    if self.peek() == '.':  # 小数部分
        self.next()
        if not self.peek().isdigit():
            raise SyntaxError("Bad decimal")
        while self.peek().isdigit():  # 读取小数部分
            self.next()
    s = self.text[j:self.i]  # 提取数字字符串
    return float(s) if '.' in s else int(s)  # 转换为数字类型
```

**作用**：读取一个 JSON 数字（整数或浮点数）

**详细流程**：

1. **记录开始位置**：`j = self.i` 用于最后提取整个数字字符串

2. **处理负号**：
   - 如果当前字符是 `-`，消耗它（表示负数）

3. **处理整数部分**：
   - 情况 1：以 `0` 开头（如 `0`, `0.5`）
     - 直接消耗 `0`
   - 情况 2：其他数字（如 `123`, `456.789`）
     - 必须至少有一个数字
     - 循环读取所有连续的数字

4. **处理小数部分**：
   - 如果遇到 `.`，表示这是浮点数
   - 必须确保 `.` 后面至少有一个数字
   - 循环读取所有小数位

5. **类型转换**：
   - 提取从 `j` 到 `i` 的子字符串
   - 如果包含 `.`，转换为 `float`
   - 否则转换为 `int`

**示例**：
```python
# "123" → 123 (int)
# "-456" → -456 (int)
# "0.5" → 0.5 (float)
# "-3.14159" → -3.14159 (float)
# "0123" → 123 (int) - 注意：JSON 不允许前导零，但这里会解析
```

**设计说明**：这个实现支持了 JSON 数字的基本格式，但简化了一些边界情况（如科学计数法）

---

### 1.6 读取关键字（read_kw）

```python
def read_kw(self, start):
    for k, v in [("true", True), ("false", False), ("null", None)]:
        if self.text.startswith(k, self.i):
            self.i += len(k)
            return v
    raise SyntaxError(f"Unknown literal near {start}")
```

**作用**：读取 JSON 关键字（`true`, `false`, `null`）

**详细流程**：

1. **尝试匹配**：依次检查当前位置是否以 `"true"`, `"false"`, `"null"` 开头

2. **使用 startswith**：`self.text.startswith(k, self.i)` 从位置 `i` 开始检查是否匹配关键字

3. **更新指针**：如果匹配，将指针向前移动关键字的长度

4. **返回值**：
   - `"true"` → Python 的 `True`
   - `"false"` → Python 的 `False`
   - `"null"` → Python 的 `None`

**示例**：
```python
# 输入：true
# 匹配到 "true"，指针移动 4 位
# 返回：True

# 输入：false
# 匹配到 "false"，指针移动 5 位
# 返回：False

# 输入：null
# 匹配到 "null"，指针移动 4 位
# 返回：None
```

**注意**：这种方法假设关键字后面不是其他字符的一部分，例如不会把 `"trueValue"` 的前 4 个字符误认为是 `true`。但在完整的 JSON 语法中，关键字后面应该有分隔符。

---

### 1.7 主方法：tokens() 生成器

```python
def tokens(self):
    while True:
        self.skip_ws()  # 跳过空白字符
        if self.i >= self.n:
            yield Token("EOF", None, self.i)  # 文件结束
            return
        ch = self.peek()
        pos = self.i
        if ch in '{}[]:,':
            self.next()
            yield Token(ch, ch, pos)  # 标点符号
        elif ch == '"':
            yield Token("STR", self.read_str(pos), pos)  # 字符串
        elif ch in '-0123456789':
            yield Token("NUM", self.read_num(pos), pos)  # 数字
        else:
            yield Token("KW", self.read_kw(pos), pos)  # 关键字
```

**作用**：主循环，生成所有 Token

**详细流程**：

1. **无限循环**：持续生成 Token 直到文件结束

2. **跳过空白**：每次循环开始都跳过空白字符

3. **检查文件结束**：
   - 如果指针超出范围，生成 `EOF` Token 并退出

4. **分类识别字符**：
   - **标点符号** (`{}[]:,`)：
     - 直接生成对应类型的 Token
     - 值就是字符本身
   - **字符串** (`"`)：
     - 调用 `read_str()` 读取完整字符串
     - 生成 `STR` 类型 Token
   - **数字** (`-` 或 `0-9`)：
     - 调用 `read_num()` 读取完整数字
     - 生成 `NUM` 类型 Token
   - **其他**：
     - 应该是关键字（true/false/null）
     - 调用 `read_kw()` 读取

5. **记录位置**：每个 Token 都记录其在源文本中的位置，便于错误报告

**示例执行流程**：
```python
# 输入：'{"name": "John", "age": 30}'
# 生成的 Token 序列：
# Token('{', '{', 0)
# Token('STR', 'name', 1)
# Token(':', ':', 6)
# Token('STR', 'John', 8)
# Token(',', ',', 14)
# Token('STR', 'age', 16)
# Token(':', ':', 21)
# Token('NUM', 30, 23)
# Token('}', '}', 25)
# Token('EOF', None, 26)
```

**设计优势**：
- 使用生成器（`yield`）：内存高效，按需生成 Token
- 统一的 Token 格式：便于后续语法分析
- 完整的错误处理：各种异常情况都有错误提示

---

## 2. JSON Parser（语法分析器）

### 2.1 Stream 类（Token 流包装）

```python
class Stream:
    def __init__(self, tokenizer):
        self.gen, self.buf = tokenizer.tokens(), []

    def peek(self):
        if not self.buf:
            self.buf.append(next(self.gen))
        return self.buf[0]

    def next(self):
        if self.buf:
            return self.buf.pop(0)
        return next(self.gen)

    def expect(self, t):
        tok = self.next()
        if tok.type != t:
            raise SyntaxError(f"Expect {t} at {tok.pos}, got {tok.type}")
        return tok
```

**作用**：为 Tokenizer 生成的 Token 流提供缓冲和前瞻能力

**为什么需要**：
- 语法分析时经常需要"看看"下一个 Token 是什么才能决定如何解析
- 但 Tokenizer 的生成器是单向的，一旦 `next()` 就消耗了 Token
- Stream 通过缓冲区实现了"回看"功能

**详细方法解析**：

#### peek() - 预览下一个 Token

```python
def peek(self):
    if not self.buf:
        self.buf.append(next(self.gen))  # 从生成器取一个 Token 放入缓冲区
    return self.buf[0]  # 返回缓冲区第一个（但不消耗）
```

**流程**：
1. 如果缓冲区为空，从生成器取一个 Token 放入缓冲区
2. 返回缓冲区第一个 Token（不消耗它）

**示例**：
```python
stream = Stream(tokenizer)
token1 = stream.peek()  # 读取第一个 Token 到缓冲区，返回它
token2 = stream.peek()  # 缓冲区已有，直接返回同一个 Token
# token1 和 token2 是同一个对象
```

#### next() - 消耗 Token

```python
def next(self):
    if self.buf:
        return self.buf.pop(0)  # 从缓冲区取（先进先出）
    return next(self.gen)  # 缓冲区为空，直接从生成器取
```

**流程**：
1. 如果缓冲区有 Token，从缓冲区取出（消耗）
2. 如果缓冲区为空，直接从生成器取

**示例**：
```python
token1 = stream.peek()   # Token A 进入缓冲区，返回 A
token2 = stream.next()   # 从缓冲区取出 A，返回 A
token3 = stream.next()   # 缓冲区为空，从生成器取下一个 Token B，返回 B
```

#### expect() - 期望特定类型的 Token

```python
def expect(self, t):
    tok = self.next()
    if tok.type != t:
        raise SyntaxError(f"Expect {t} at {tok.pos}, got {tok.type}")
    return tok
```

**作用**：确保下一个 Token 是期望的类型，否则抛出错误

**使用场景**：在解析 JSON 对象时，我们知道应该有 `{` 开始，可以：
```python
stream.expect('{')  # 确保是左花括号，否则报错
```

---

### 2.2 Parser 类 - 核心解析方法

#### parse() - 入口方法

```python
class Parser:
    def parse(self, text):
        ts = Stream(Tokenizer(text))
        val = self.value(ts)
        if ts.peek().type != "EOF":
            raise SyntaxError("Extra content after JSON")
        return val
```

**作用**：解析完整的 JSON 文本

**流程**：
1. 创建 Tokenizer，包装成 Stream
2. 调用 `value()` 解析 JSON 值（可以是对象、数组、字符串、数字等）
3. 检查是否还有多余内容（应该是 EOF）
4. 返回解析结果

---

#### value() - 解析 JSON 值

```python
def value(self, ts):
    t = ts.peek()  # 预览下一个 Token
    if t.type == '{': return self.obj(ts)      # 对象
    if t.type == '[': return self.arr(ts)      # 数组
    if t.type == 'STR': return ts.next().value # 字符串
    if t.type == 'NUM': return ts.next().value # 数字
    if t.type == 'KW':  return ts.next().value # 关键字（true/false/null）
    raise SyntaxError(f"Unexpected token {t.type}")
```

**作用**：根据 Token 类型，分发到相应的解析方法

**设计模式**：这是典型的递归下降解析器的根方法，根据第一个 Token 的类型决定解析路径

**JSON 值类型**：
- 对象：`{...}`
- 数组：`[...]`
- 字符串：`"..."`
- 数字：`123`, `3.14`
- 关键字：`true`, `false`, `null`

---

#### obj() - 解析 JSON 对象

```python
def obj(self, ts):
    ts.expect('{')  # 确保是左花括号
    o = {}  # 创建空字典
    
    if ts.peek().type == '}':  # 空对象
        ts.next()
        return o
    
    while True:
        k = ts.expect("STR").value  # 键必须是字符串
        ts.expect(':')              # 键值分隔符
        o[k] = self.value(ts)       # 递归解析值
        
        t = ts.peek()  # 查看下一个 Token
        if t.type == ',':
            ts.next()  # 消耗逗号，继续下一个键值对
        elif t.type == '}':
            ts.next()  # 消耗右花括号，结束
            break
        else:
            raise SyntaxError(f"Unexpected {t.type}")
    return o
```

**作用**：解析 JSON 对象，格式为 `{"key1": value1, "key2": value2}`

**详细流程**：

1. **期望左花括号**：`ts.expect('{')` 确保对象开始

2. **处理空对象**：
   ```json
   {}
   ```
   - 如果下一个 Token 是 `}`，直接返回空字典

3. **循环解析键值对**：
   ```json
   {"name": "John", "age": 30}
   ```
   - **读取键**：必须是字符串类型（`ts.expect("STR")`）
   - **期望冒号**：键和值之间必须有 `:`
   - **递归解析值**：`self.value(ts)` 可以解析任何类型的值（嵌套对象、数组等）
   - **存储键值对**：`o[k] = ...`

4. **处理分隔符**：
   - 如果是 `,`：继续读取下一个键值对
   - 如果是 `}`：对象结束，跳出循环
   - 其他：语法错误

**递归特性**：
```json
{
  "user": {
    "name": "John",
    "address": {
      "city": "New York"
    }
  }
}
```
- 解析外层对象时，遇到 `"user"` 的值是对象，递归调用 `self.value()` → `self.obj()`

**示例执行**：
```python
# 输入：'{"name": "John", "age": 30}'
# 流程：
# 1. expect('{') → 消耗 {
# 2. expect("STR") → 键 "name"
# 3. expect(':') → 消耗 :
# 4. value() → 递归解析 "John"，返回字符串
# 5. o["name"] = "John"
# 6. peek() → 看到 ','，消耗它
# 7. expect("STR") → 键 "age"
# 8. expect(':') → 消耗 :
# 9. value() → 解析 30，返回整数
# 10. o["age"] = 30
# 11. peek() → 看到 '}'，消耗它，break
# 结果：{"name": "John", "age": 30}
```

---

#### arr() - 解析 JSON 数组

```python
def arr(self, ts):
    ts.expect('[')  # 期望左方括号
    a = []  # 创建空列表
    
    if ts.peek().type == ']':  # 空数组
        ts.next()
        return a
    
    while True:
        a.append(self.value(ts))  # 递归解析元素
        t = ts.peek()
        if t.type == ',':
            ts.next()  # 消耗逗号，继续下一个元素
        elif t.type == ']':
            ts.next()  # 消耗右方括号，结束
            break
        else:
            raise SyntaxError(f"Unexpected {t.type}")
    return a
```

**作用**：解析 JSON 数组，格式为 `[value1, value2, value3]`

**详细流程**：

1. **期望左方括号**：`ts.expect('[')`

2. **处理空数组**：
   ```json
   []
   ```

3. **循环解析元素**：
   ```json
   [1, "hello", true, {"key": "value"}]
   ```
   - 每个元素通过 `self.value(ts)` 递归解析
   - 元素可以是任何 JSON 值类型（包括嵌套数组和对象）

4. **处理分隔符**：
   - `,`：继续下一个元素
   - `]`：数组结束

**示例执行**：
```python
# 输入：'[1, "hello", true]'
# 流程：
# 1. expect('[') → 消耗 [
# 2. value() → 解析 1，添加到数组
# 3. peek() → 看到 ','，消耗它
# 4. value() → 解析 "hello"，添加到数组
# 5. peek() → 看到 ','，消耗它
# 6. value() → 解析 true，添加到数组
# 7. peek() → 看到 ']'，消耗它，break
# 结果：[1, "hello", True]
```

---

### 2.3 解析器总结

**设计模式**：递归下降解析器（Recursive Descent Parser）

**特点**：
- 每个语法结构（对象、数组）对应一个方法
- 方法内部可以递归调用其他方法（处理嵌套）
- 使用前瞻（peek）来决定解析路径
- 使用期望（expect）来验证语法正确性

**错误处理**：
- 每个方法都会在语法不正确时抛出 `SyntaxError`
- 错误信息包含位置信息，便于调试

**解析能力**：
- ✅ 嵌套对象：`{"a": {"b": {"c": 1}}}`
- ✅ 嵌套数组：`[[1, 2], [3, 4]]`
- ✅ 混合嵌套：`{"items": [{"id": 1}, {"id": 2}]}`
- ✅ 所有基本类型：字符串、数字、布尔值、null

---

## 3. Collection 类（数据集合操作）

### 3.1 类初始化

```python
class Collection:
    def __init__(self, data):
        self.data = data if isinstance(data, list) else [data]
```

**作用**：创建一个数据集合，统一处理单个对象或对象列表

**设计思路**：
- 统一接口：无论输入是单个文档还是文档列表，都转换为列表
- 方便后续操作：所有查询操作都假设 `self.data` 是列表

**示例**：
```python
# 单个对象 → 转换为列表
coll1 = Collection({"name": "John"})
# self.data = [{"name": "John"}]

# 列表 → 保持不变
coll2 = Collection([{"name": "John"}, {"name": "Jane"}])
# self.data = [{"name": "John"}, {"name": "Jane"}]
```

---

### 3.2 _extract_key() - 键提取辅助方法

```python
def _extract_key(self, doc, key):
    """supports dot notation"""
    ks = key.split(".")  # 按点号分割键路径
    cur = doc  # 当前查找位置
    for k in ks:
        if not isinstance(cur, dict) or k not in cur:
            return None  # 路径不存在
        cur = cur[k]  # 进入下一层
    return cur
```

**作用**：从文档中提取字段值，支持点号表示法访问嵌套字段

**详细流程**：

1. **分割键路径**：`"user.name"` → `["user", "name"]`

2. **逐层查找**：
   - 从文档根开始
   - 每层检查是否为字典且包含键
   - 进入下一层

3. **错误处理**：如果路径不存在，返回 `None`

**示例**：
```python
doc = {
    "user": {
        "name": "John",
        "age": 30
    },
    "city": "NYC"
}

# 简单键
_extract_key(doc, "city")  # → "NYC"

# 嵌套键
_extract_key(doc, "user.name")  # → "John"
_extract_key(doc, "user.age")   # → 30

# 不存在的键
_extract_key(doc, "user.email")  # → None
_extract_key(doc, "invalid.path")  # → None
```

**复杂嵌套示例**：
```python
doc = {
    "level1": {
        "level2": {
            "level3": {
                "value": 42
            }
        }
    }
}

_extract_key(doc, "level1.level2.level3.value")  # → 42
```

**设计优势**：
- 统一接口：无论是简单字段还是嵌套字段，都使用相同的方法
- 容错性：路径不存在时返回 `None`，不会抛出异常
- 灵活性：支持任意深度的嵌套

---

### 3.3 find() - 查询过滤

```python
def find(self, query=None):
    if query is None:  # 没有查询条件
        return self.data  # 返回所有数据
    
    def match(doc, query):  # 内部函数：检查文档是否匹配查询
        for key, value in query.items():
            cur = self._extract_key(doc, key)  # 提取字段值
            if cur != value:
                return False  # 有一个字段不匹配就返回 False
        return True  # 所有字段都匹配
    
    return [doc for doc in self.data if match(doc, query)]
```

**作用**：根据查询条件过滤文档，类似 SQL 的 `WHERE` 子句

**详细解析**：

#### 无查询条件
```python
collection.find()  # 返回所有文档
collection.find(None)  # 同上
```

#### 有查询条件
```python
# 查询：{"name": "John", "age": 30}
# 匹配的文档：所有 name="John" 且 age=30 的文档
```

**match() 内部函数**：
- 遍历查询条件的每个键值对
- 从文档中提取对应字段的值
- 如果值不相等，文档不匹配
- 只有所有条件都满足，才返回 `True`

**列表推导式**：
```python
[doc for doc in self.data if match(doc, query)]
```
- 遍历所有文档
- 只保留匹配的文档

**示例**：
```python
data = [
    {"name": "John", "age": 30, "city": "NYC"},
    {"name": "Jane", "age": 25, "city": "LA"},
    {"name": "John", "age": 30, "city": "SF"}
]

coll = Collection(data)

# 单个条件
coll.find({"name": "John"})
# 结果：[{"name": "John", "age": 30, "city": "NYC"},
#        {"name": "John", "age": 30, "city": "SF"}]

# 多个条件（AND 关系）
coll.find({"name": "John", "age": 30})
# 结果：[{"name": "John", "age": 30, "city": "NYC"},
#        {"name": "John", "age": 30, "city": "SF"}]

# 无匹配
coll.find({"name": "Bob"})
# 结果：[]

# 无查询条件
coll.find()
# 结果：所有三个文档
```

**支持嵌套字段**：
```python
data = [
    {"user": {"name": "John"}, "age": 30},
    {"user": {"name": "Jane"}, "age": 25}
]

coll = Collection(data)
coll.find({"user.name": "John"})  # 使用点号表示法
# 结果：[{"user": {"name": "John"}, "age": 30}]
```

**限制**：
- 只支持等值匹配（`==`）
- 不支持范围查询（`>`, `<`）
- 不支持 OR 条件
- 多个条件是 AND 关系

---

### 3.4 project() - 字段投影

```python
def project(self, fields):
    """Return documents with only selected fields."""
    result = []
    for doc in self.data:
        projected = {}  # 新文档，只包含选中字段
        for field in fields:
            # 使用 _extract_key 处理嵌套键
            projected[field] = self._extract_key(doc, field)
        result.append(projected)
    return result
```

**作用**：选择要显示的字段，类似 SQL 的 `SELECT field1, field2 FROM ...`

**详细流程**：

1. **遍历每个文档**

2. **创建投影文档**：
   - 只包含 `fields` 中指定的字段
   - 使用 `_extract_key()` 提取值（支持嵌套字段）

3. **添加到结果列表**

**示例**：
```python
data = [
    {"name": "John", "age": 30, "city": "NYC", "salary": 50000},
    {"name": "Jane", "age": 25, "city": "LA", "salary": 60000}
]

coll = Collection(data)

# 选择单个字段
coll.project(["name"])
# 结果：[{"name": "John"}, {"name": "Jane"}]

# 选择多个字段
coll.project(["name", "age"])
# 结果：[{"name": "John", "age": 30}, {"name": "Jane", "age": 25}]

# 嵌套字段
data2 = [
    {"user": {"name": "John", "email": "john@example.com"}, "age": 30}
]
coll2 = Collection(data2)
coll2.project(["user.name", "age"])
# 结果：[{"user.name": "John", "age": 30}]
```

**设计特点**：
- 字段名保持不变：如果选择 `"user.name"`，结果中也使用 `"user.name"` 作为键
- 缺失字段返回 `None`：如果文档中没有某个字段，值为 `None`
- 支持嵌套字段：通过点号表示法选择嵌套字段

**与 find() 结合**：
```python
# 先过滤，再投影
filtered = coll.find({"city": "NYC"})
projected = Collection(filtered).project(["name", "age"])
```

---

### 3.5 groupby() - 分组

```python
def groupby(self, key):
    groups = {}  # 字典：分组值 -> 文档列表
    for doc in self.data:
        group_value = self._extract_key(doc, key)  # 提取分组键的值
        groups.setdefault(group_value, []).append(doc)
    return groups
```

**作用**：按指定字段的值对文档进行分组，类似 SQL 的 `GROUP BY`

**详细流程**：

1. **创建分组字典**：键是分组字段的值，值是该组的所有文档列表

2. **遍历文档**：
   - 提取分组字段的值
   - 将该文档添加到对应组的列表中

3. **setdefault 用法**：
   ```python
   groups.setdefault(group_value, []).append(doc)
   ```
   - 如果 `group_value` 不在字典中，创建新列表 `[]`
   - 如果已存在，使用现有列表
   - 将文档添加到列表中

**示例**：
```python
data = [
    {"city": "NYC", "name": "John", "age": 30},
    {"city": "NYC", "name": "Jane", "age": 25},
    {"city": "LA", "name": "Bob", "age": 35},
    {"city": "LA", "name": "Alice", "age": 28}
]

coll = Collection(data)
groups = coll.groupby("city")

# 结果：
# {
#     "NYC": [
#         {"city": "NYC", "name": "John", "age": 30},
#         {"city": "NYC", "name": "Jane", "age": 25}
#     ],
#     "LA": [
#         {"city": "LA", "name": "Bob", "age": 35},
#         {"city": "LA", "name": "Alice", "age": 28}
#     ]
# }
```

**嵌套字段分组**：
```python
data = [
    {"user": {"city": "NYC"}, "name": "John"},
    {"user": {"city": "NYC"}, "name": "Jane"},
    {"user": {"city": "LA"}, "name": "Bob"}
]

coll = Collection(data)
groups = coll.groupby("user.city")
# 按嵌套字段分组
```

**空值处理**：
```python
data = [
    {"city": "NYC", "name": "John"},
    {"city": None, "name": "Jane"}  # 缺少字段
]

coll = Collection(data)
groups = coll.groupby("city")
# {
#     "NYC": [{"city": "NYC", "name": "John"}],
#     None: [{"city": None, "name": "Jane"}]
# }
```

---

### 3.6 aggregate() - 聚合

```python
def aggregate(self, group_key, agg_func):
    """Apply an aggregation function (sum, count, avg, etc.) on each group."""
    grouped = self.groupby(group_key)  # 先分组
    result = {}
    for k, docs in grouped.items():
        result[k] = agg_func(docs)  # 对每组应用聚合函数
    return result
```

**作用**：对分组后的数据进行聚合计算，类似 SQL 的 `GROUP BY ... COUNT/SUM/AVG(...)`

**详细流程**：

1. **先分组**：调用 `groupby()` 得到分组字典

2. **对每组应用聚合函数**：
   - 遍历每个分组
   - 调用 `agg_func(docs)` 对组内文档进行聚合
   - 结果存储为 `{分组值: 聚合结果}`

**聚合函数**：接受文档列表，返回聚合值（见第 4 节）

**示例**：
```python
data = [
    {"city": "NYC", "sales": 100},
    {"city": "NYC", "sales": 200},
    {"city": "LA", "sales": 150},
    {"city": "LA", "sales": 250}
]

coll = Collection(data)

# 计算每组的数量
count_func = lambda docs: len(docs)
result = coll.aggregate("city", count_func)
# 结果：{"NYC": 2, "LA": 2}

# 计算每组的销售额总和
sum_func = lambda docs: sum(doc["sales"] for doc in docs)
result = coll.aggregate("city", sum_func)
# 结果：{"NYC": 300, "LA": 400}
```

**与聚合函数库结合**（见第 4 节）：
```python
from aggregate_functions import agg_count, agg_sum

coll.aggregate("city", agg_count())  # 计数
coll.aggregate("city", agg_sum("sales"))  # 求和
```

---

### 3.7 hash_join() - 哈希连接

```python
def hash_join(self, other, key_self, key_other, join_type="inner"):
    """
    join_type: inner / left / right / full
    """
    
    # 步骤 1：为右表（other）构建哈希表
    hashmap = {}
    for doc in other.data:
        val = self._extract_key(doc, key_other)  # 提取连接键的值
        hashmap.setdefault(val, []).append(doc)  # 相同键值的文档放入同一列表
    
    result = []
    matched_right_keys = set()  # 记录已匹配的右表文档（用于 right/full join）
    
    # 步骤 2：处理左表（self）的每个文档
    for doc_left in self.data:
        val_left = self._extract_key(doc_left, key_self)
        if val_left in hashmap:  # 在哈希表中找到匹配
            for doc_right in hashmap[val_left]:  # 可能有多个匹配（一对多）
                matched_right_keys.add(id(doc_right))  # 记录已匹配
                result.append({
                    "left": doc_left,
                    "right": doc_right
                })
        else:  # 左表文档没有匹配
            if join_type in ("left", "full"):
                result.append({
                    "left": doc_left,
                    "right": None  # 右表字段为 None
                })
    
    # 步骤 3：处理未匹配的右表文档（right join 或 full join）
    if join_type in ("right", "full"):
        for doc_right in other.data:
            if id(doc_right) not in matched_right_keys:
                result.append({
                    "left": None,  # 左表字段为 None
                    "right": doc_right
                })
    
    return result
```

**作用**：实现哈希连接算法，连接两个数据集，类似 SQL 的 `JOIN`

**连接类型**：
- **inner join**：只返回两表都有匹配的记录
- **left join**：返回左表所有记录，右表无匹配时用 `None`
- **right join**：返回右表所有记录，左表无匹配时用 `None`
- **full join**：返回两表所有记录，无匹配时用 `None`

**详细算法解析**：

#### 步骤 1：构建哈希表（右表）

```python
hashmap = {}
for doc in other.data:
    val = self._extract_key(doc, key_other)
    hashmap.setdefault(val, []).append(doc)
```

**目的**：快速查找匹配的右表文档

**结构**：
```python
{
    连接键值1: [文档1, 文档2, ...],  # 可能有多个文档有相同键值（一对多）
    连接键值2: [文档3, ...],
    ...
}
```

**时间复杂度**：O(n)，n 是右表文档数

#### 步骤 2：处理左表（内连接 + 左连接部分）

```python
for doc_left in self.data:
    val_left = self._extract_key(doc_left, key_self)
    if val_left in hashmap:  # 找到匹配
        for doc_right in hashmap[val_left]:
            result.append({"left": doc_left, "right": doc_right})
    else:  # 未找到匹配
        if join_type in ("left", "full"):
            result.append({"left": doc_left, "right": None})
```

**流程**：
1. 遍历左表每个文档
2. 提取连接键值
3. 在哈希表中查找
   - **有匹配**：生成所有匹配对（支持一对多）
   - **无匹配**：如果是 left/full join，添加左表记录，右表为 `None`

#### 步骤 3：处理未匹配的右表文档（右连接 + 全连接部分）

```python
if join_type in ("right", "full"):
    for doc_right in other.data:
        if id(doc_right) not in matched_right_keys:
            result.append({"left": None, "right": doc_right})
```

**目的**：添加右表中没有匹配的文档

**使用 `id()`**：Python 对象的唯一标识符，用于判断文档是否已被匹配

**示例执行**：

```python
# 左表
left_data = [
    {"id": 1, "name": "John"},
    {"id": 2, "name": "Jane"},
    {"id": 3, "name": "Bob"}
]

# 右表
right_data = [
    {"user_id": 1, "city": "NYC"},
    {"user_id": 1, "city": "SF"},  # 一对多
    {"user_id": 2, "city": "LA"}
]

left_coll = Collection(left_data)
right_coll = Collection(right_data)

# Inner Join
result = left_coll.hash_join(right_coll, "id", "user_id", "inner")
# 结果：
# [
#     {"left": {"id": 1, "name": "John"}, "right": {"user_id": 1, "city": "NYC"}},
#     {"left": {"id": 1, "name": "John"}, "right": {"user_id": 1, "city": "SF"}},  # 一对多
#     {"left": {"id": 2, "name": "Jane"}, "right": {"user_id": 2, "city": "LA"}}
# ]
# 注意：id=3 的 Bob 没有匹配，不包含在结果中

# Left Join
result = left_coll.hash_join(right_coll, "id", "user_id", "left")
# 结果包含 id=3 的记录，right 为 None

# Right Join
result = left_coll.hash_join(right_coll, "id", "user_id", "right")
# 结果包含所有右表记录，如果左表没有匹配，left 为 None

# Full Join
result = left_coll.hash_join(right_coll, "id", "user_id", "full")
# 结果包含两表所有记录
```

**算法优势**：
- **时间复杂度**：O(m + n)，m 和 n 是两表大小（哈希表查找是 O(1)）
- **空间复杂度**：O(n)，需要存储右表哈希表
- **高效**：比嵌套循环 O(m × n) 快得多

**限制**：
- 只支持等值连接（相等匹配）
- 不支持范围连接（`>`, `<`）

---

### 3.8 pipeline() - 操作管道

```python
def pipeline(self, query=None, project_fields=None,
             group_key=None, agg_func=None,
             join_collection=None, join_self_key=None, 
             join_other_key=None, join_type="inner"):
    data = self.data
    
    # 步骤 1：过滤（如果有查询条件）
    if query:
        data = Collection(data).find(query)
    
    # 步骤 2：投影（如果指定了字段）
    if project_fields:
        data = Collection(data).project(project_fields)
    
    # 步骤 3：聚合（如果指定了分组和聚合函数）
    if group_key and agg_func:
        data = Collection(data).aggregate(group_key, agg_func)
    
    # 步骤 4：连接（如果指定了连接表）
    if join_collection:
        data = Collection(data).hash_join(
            join_collection,
            join_self_key,
            join_other_key,
            join_type
        )
    
    return data
```

**作用**：将多个操作组合成一个管道，按顺序执行

**操作顺序**：
1. **find**（过滤）：减少数据量
2. **project**（投影）：选择字段
3. **aggregate**（聚合）：分组统计
4. **hash_join**（连接）：合并数据

**设计思路**：
- 每个操作的结果作为下一个操作的输入
- 使用 `Collection(data)` 包装中间结果，保持接口一致

**示例**：
```python
# 查询 city="NYC" 的记录，选择 name 和 age 字段，按 age 分组统计数量
result = coll.pipeline(
    query={"city": "NYC"},
    project_fields=["name", "age"],
    group_key="age",
    agg_func=agg_count()
)
```

**实际执行流程**：
```python
# 原始数据
data = [
    {"name": "John", "age": 30, "city": "NYC"},
    {"name": "Jane", "age": 25, "city": "NYC"},
    {"name": "Bob", "age": 30, "city": "LA"}
]

# 步骤 1：find({"city": "NYC"})
filtered = [
    {"name": "John", "age": 30, "city": "NYC"},
    {"name": "Jane", "age": 25, "city": "NYC"}
]

# 步骤 2：project(["name", "age"])
projected = [
    {"name": "John", "age": 30},
    {"name": "Jane", "age": 25}
]

# 步骤 3：aggregate("age", agg_count())
result = {
    30: 1,  # 有 1 个 30 岁的
    25: 1   # 有 1 个 25 岁的
}
```

**灵活性**：
- 所有参数都是可选的
- 可以只使用部分操作
- 顺序固定（优化过的顺序）

---

## 4. 聚合函数实现

### 4.1 agg_count() - 计数

```python
def agg_count(field=None):
    return lambda docs: len(docs)
```

**作用**：统计文档数量（分组中的文档数）

**设计模式**：高阶函数
- `agg_count()` 返回一个函数（lambda）
- 这个函数接受文档列表，返回数量

**为什么这样设计**：
- 统一接口：所有聚合函数都返回 `(docs) -> value` 的函数
- 灵活：虽然 `field` 参数在这里不用，但保持了接口一致性

**使用示例**：
```python
count_func = agg_count()  # 返回函数
result = coll.aggregate("city", count_func)  # 对每组应用函数

# 等价写法
result = coll.aggregate("city", lambda docs: len(docs))
```

**结果示例**：
```python
data = [
    {"city": "NYC", "name": "John"},
    {"city": "NYC", "name": "Jane"},
    {"city": "LA", "name": "Bob"}
]

coll = Collection(data)
result = coll.aggregate("city", agg_count())
# 结果：{"NYC": 2, "LA": 1}
```

---

### 4.2 agg_sum() - 求和

```python
def agg_sum(field):
    return lambda docs: sum(
        doc.get(field, 0) for doc in docs
        if isinstance(doc.get(field), (int, float))
    )
```

**作用**：对指定字段求和

**详细解析**：

1. **返回函数**：lambda 接受文档列表

2. **生成器表达式**：
   ```python
   doc.get(field, 0) for doc in docs
       if isinstance(doc.get(field), (int, float))
   ```
   - 遍历每个文档
   - 只处理数值类型（int 或 float）
   - 使用 `doc.get(field, 0)` 安全获取字段值，缺失时默认为 0

3. **求和**：`sum(...)` 对生成器结果求和

**示例**：
```python
data = [
    {"city": "NYC", "sales": 100},
    {"city": "NYC", "sales": 200},
    {"city": "LA", "sales": 150},
    {"city": "LA", "sales": None}  # 非数值，会被跳过
]

coll = Collection(data)
result = coll.aggregate("city", agg_sum("sales"))
# 结果：{"NYC": 300, "LA": 150}
```

**容错处理**：
- 缺失字段：使用默认值 0
- 非数值字段：通过 `isinstance` 检查，跳过
- 类型安全：只处理数字类型

---

### 4.3 agg_max() - 最大值

```python
def agg_max(field):
    return lambda docs: max(
        doc.get(field) for doc in docs
        if isinstance(doc.get(field), (int, float))
    )
```

**作用**：找到指定字段的最大值

**注意**：
- 使用 `doc.get(field)` 而不是 `doc.get(field, 0)`
- 因为如果所有文档都缺失该字段，`max()` 会抛出错误（空序列）
- 但这里通过 `if` 条件过滤，确保生成器至少有一个值

**示例**：
```python
data = [
    {"city": "NYC", "sales": 100},
    {"city": "NYC", "sales": 200},
    {"city": "LA", "sales": 150}
]

coll = Collection(data)
result = coll.aggregate("city", agg_max("sales"))
# 结果：{"NYC": 200, "LA": 150}
```

**边界情况**：
- 如果组内所有文档都缺失该字段，生成器为空，`max()` 会抛出 `ValueError`
- 实际使用中可能需要额外处理

---

### 4.4 agg_min() - 最小值

```python
def agg_min(field):
    return lambda docs: min(
        doc.get(field) for doc in docs
        if isinstance(doc.get(field), (int, float))
    )
```

**作用**：找到指定字段的最小值

**实现**：与 `agg_max()` 类似，使用 `min()` 代替 `max()`

---

### 4.5 agg_avg() - 平均值

```python
def agg_avg(field):
    return lambda docs: (
        sum(doc.get(field, 0) for doc in docs
            if isinstance(doc.get(field), (int, float)))
        / len(docs)
        if docs else None
    )
```

**作用**：计算指定字段的平均值

**详细解析**：

1. **求和部分**：
   ```python
   sum(doc.get(field, 0) for doc in docs
       if isinstance(doc.get(field), (int, float)))
   ```
   - 只对数值字段求和
   - 缺失字段使用 0

2. **除以文档数**：
   ```python
   / len(docs)
   ```
   - 使用总文档数（不是有效值的数量）
   - 这意味着缺失值被当作 0 参与计算

3. **空列表处理**：
   ```python
   if docs else None
   ```
   - 如果文档列表为空，返回 `None`（避免除零错误）

**示例**：
```python
data = [
    {"city": "NYC", "sales": 100},
    {"city": "NYC", "sales": 200},
    {"city": "NYC", "sales": None}  # 缺失值
]

coll = Collection(data)
result = coll.aggregate("city", agg_avg("sales"))
# 计算：(100 + 200 + 0) / 3 = 100.0
# 注意：缺失值被当作 0，分母是总文档数 3
```

**设计选择**：
- 使用 `len(docs)` 作为分母：包含所有文档（缺失值计为 0）
- 另一种设计：只计算有效值的平均值，分母是有效值数量
- 当前实现更简单，但可能不符合某些业务需求

---

## 5. 分块处理机制

### 5.1 load_json_chunks() - 分块加载器

```python
def load_json_chunks(path, chunk_size=5000):
    """
    Generic loader:
        - if JSONL:  one JSON object per line
        - if JSON array: [ {...}, {...} ]
    """
    with open(path, "r", encoding="utf-8") as f:
        first_char = f.read(1)  # 读取第一个字符，判断格式
        f.seek(0)  # 回到文件开头
        
        if first_char == "[":  # JSON 数组格式
            text = f.read()  # 读取整个文件
            parser = Parser()
            arr = parser.parse(text)  # 解析为数组
            for i in range(0, len(arr), chunk_size):
                yield arr[i:i + chunk_size]  # 按块大小切片
        else:  # JSONL 格式（每行一个 JSON 对象）
            parser = Parser()
            buffer = []  # 缓冲区
            for line in f:
                line = line.strip()  # 去除首尾空白
                if not line:
                    continue  # 跳过空行
                buffer.append(parser.parse(line))  # 解析一行，加入缓冲区
                if len(buffer) >= chunk_size:  # 缓冲区满了
                    yield buffer  # 返回一个块
                    buffer = []  # 清空缓冲区
            if buffer:  # 处理剩余数据
                yield buffer
```

**作用**：将大型 JSON/JSONL 文件分块加载，避免内存溢出

**为什么需要**：
- 大型文件（GB 级别）无法一次性加载到内存
- 分块处理可以逐块处理，内存占用可控

**详细流程**：

#### 格式识别

```python
first_char = f.read(1)  # 读取第一个字符
f.seek(0)  # 重置文件指针
```

**判断逻辑**：
- `[` → JSON 数组格式：`[{...}, {...}, ...]`
- 其他 → JSONL 格式：每行一个 JSON 对象

---

#### JSON 数组格式处理

```python
if first_char == "[":
    text = f.read()  # 读取整个文件到内存
    parser = Parser()
    arr = parser.parse(text)  # 解析为 Python 列表
    for i in range(0, len(arr), chunk_size):
        yield arr[i:i + chunk_size]  # 切片生成块
```

**流程**：
1. 读取整个文件（假设文件不是太大）
2. 解析为 Python 列表
3. 按 `chunk_size` 切片，生成器返回每个切片

**限制**：
- 文件必须先全部加载到内存
- 如果文件太大（超过内存），会失败
- 更适合中等大小的 JSON 数组文件

**示例**：
```python
# 文件内容：[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
# chunk_size = 3

# 生成的块：
# [1, 2, 3]
# [4, 5, 6]
# [7, 8, 9]
# [10]
```

---

#### JSONL 格式处理

```python
else:  # JSONL
    parser = Parser()
    buffer = []
    for line in f:
        line = line.strip()
        if not line:
            continue
        buffer.append(parser.parse(line))
        if len(buffer) >= chunk_size:
            yield buffer
            buffer = []
    if buffer:
        yield buffer
```

**流程**：

1. **逐行读取**：不一次性加载整个文件
   - 内存友好：每次只处理一行

2. **解析每行**：
   - 每行是一个独立的 JSON 对象
   - 使用自定义 Parser 解析

3. **缓冲累积**：
   - 将解析后的对象加入缓冲区
   - 当缓冲区达到 `chunk_size` 时，生成一个块

4. **处理剩余**：
   - 文件末尾可能还有不足 `chunk_size` 的数据
   - 需要单独处理

**优势**：
- ✅ 内存高效：不需要加载整个文件
- ✅ 支持超大文件（GB 级别）
- ✅ 逐行处理，内存占用 = `chunk_size` × 平均文档大小

**示例**：
```python
# 文件内容（JSONL）：
# {"id": 1}
# {"id": 2}
# {"id": 3}
# {"id": 4}
# {"id": 5}
# chunk_size = 2

# 生成的块：
# [{"id": 1}, {"id": 2}]
# [{"id": 3}, {"id": 4}]
# [{"id": 5}]
```

**生成器优势**：
- 使用 `yield` 而不是 `return`，生成器函数
- 按需生成块，不一次性生成所有块
- 内存占用更小

---

### 5.2 分块处理的应用场景

**典型场景**：处理社交媒体数据文件
- 文件大小：几 GB 到几十 GB
- 记录数：数百万到数千万
- 无法一次性加载到内存

**解决方案**：
1. 分块加载文件
2. 对每个块进行局部聚合
3. 合并所有块的聚合结果

**示例代码结构**：
```python
# 初始化全局聚合容器
global_results = {}

# 分块处理
for chunk in load_json_chunks("large_file.jsonl", chunk_size=5000):
    # 处理当前块
    chunk_results = process_chunk(chunk)
    
    # 合并到全局结果
    global_results = merge(global_results, chunk_results)
```

---

## 6. 部分聚合合并

### 6.1 PartialAgg 类

```python
class PartialAgg:
    """merge of partial aggregation results"""
    
    @staticmethod
    def merge_count(v1, v2):
        return v1 + v2
    
    @staticmethod
    def merge_sum(v1, v2):
        return v1 + v2
    
    @staticmethod
    def merge_max(v1, v2):
        return max(v1, v2)
    
    @staticmethod
    def merge_min(v1, v2):
        return min(v1, v2)
    
    @staticmethod
    def merge_avg(avg1, count1, avg2, count2):
        # weighted average
        total = count1 + count2
        return (avg1 * count1 + avg2 * count2) / total, total
```

**作用**：合并来自不同数据块的聚合结果

**为什么需要**：
- 当数据分块处理时，每个块产生部分聚合结果
- 需要正确合并这些部分结果，得到全局聚合结果

**详细解析**：

---

#### merge_count() - 合并计数

```python
@staticmethod
def merge_count(v1, v2):
    return v1 + v2
```

**逻辑**：计数可以直接相加

**示例**：
```python
# 块 1 的计数：{"NYC": 5, "LA": 3}
# 块 2 的计数：{"NYC": 4, "LA": 2}
# 合并结果：{"NYC": 9, "LA": 5}
```

---

#### merge_sum() - 合并求和

```python
@staticmethod
def merge_sum(v1, v2):
    return v1 + v2
```

**逻辑**：求和也可以直接相加

**示例**：
```python
# 块 1 的销售额总和：{"NYC": 1000, "LA": 800}
# 块 2 的销售额总和：{"NYC": 1200, "LA": 900}
# 合并结果：{"NYC": 2200, "LA": 1700}
```

---

#### merge_max() - 合并最大值

```python
@staticmethod
def merge_max(v1, v2):
    return max(v1, v2)
```

**逻辑**：取两个值的最大值

**示例**：
```python
# 块 1 的最大销售额：{"NYC": 500, "LA": 400}
# 块 2 的最大销售额：{"NYC": 600, "LA": 350}
# 合并结果：{"NYC": 600, "LA": 400}
```

---

#### merge_min() - 合并最小值

```python
@staticmethod
def merge_min(v1, v2):
    return min(v1, v2)
```

**逻辑**：取两个值的最小值

---

#### merge_avg() - 合并平均值（最复杂）

```python
@staticmethod
def merge_avg(avg1, count1, avg2, count2):
    # weighted average
    total = count1 + count2
    return (avg1 * count1 + avg2 * count2) / total, total
```

**问题**：平均值不能直接平均！

**错误示例**：
```python
# 块 1：平均销售额 100，有 10 条记录
# 块 2：平均销售额 200，有 20 条记录
# 错误合并：(100 + 200) / 2 = 150 ❌
# 正确应该是：(100*10 + 200*20) / (10+20) = 166.67 ✅
```

**正确方法**：加权平均

1. **需要的信息**：
   - `avg1`, `count1`：第一个块的平均值和记录数
   - `avg2`, `count2`：第二个块的平均值和记录数

2. **计算公式**：
   ```
   全局平均值 = (平均值1 × 数量1 + 平均值2 × 数量2) / (数量1 + 数量2)
   ```

3. **数学原理**：
   ```
   总和1 = 平均值1 × 数量1
   总和2 = 平均值2 × 数量2
   全局总和 = 总和1 + 总和2
   全局平均值 = 全局总和 / (数量1 + 数量2)
   ```

**返回值**：
- 返回两个值：`(合并后的平均值, 总记录数)`
- 总记录数需要保留，用于后续合并

**示例**：
```python
# 块 1：NYC 平均销售额 100，5 条记录
# 块 2：NYC 平均销售额 150，4 条记录

avg, count = PartialAgg.merge_avg(100, 5, 150, 4)
# avg = (100*5 + 150*4) / (5+4) = 122.22
# count = 9
```

**多块合并**：
```python
# 初始
avg, count = 100, 5

# 合并块 2
avg, count = PartialAgg.merge_avg(avg, count, 150, 4)
# 结果：122.22, 9

# 合并块 3
avg, count = PartialAgg.merge_avg(avg, count, 120, 6)
# 结果：((122.22*9) + (120*6)) / (9+6) = 121.33, 15
```

---

### 6.2 Map-Reduce 模式

**Map-Reduce 是分布式计算的经典模式**：

1. **Map 阶段**（映射）：
   - 将大数据集分割成多个块
   - 对每个块独立进行局部聚合计算
   - 输出部分聚合结果

2. **Reduce 阶段**（归约）：
   - 收集所有部分聚合结果
   - 使用合并函数合并结果
   - 输出最终全局结果

**本项目中的应用**：

```python
# Map 阶段：每个块独立聚合
for chunk in load_json_chunks(filepath):
    chunk_results = aggregate(chunk)  # 局部聚合
    partial_results.append(chunk_results)

# Reduce 阶段：合并所有局部结果
final_results = merge_all(partial_results)  # 全局合并
```

**优势**：
- 可并行化：每个块的 Map 操作可以并行执行
- 内存友好：不需要加载整个数据集
- 可扩展：可以处理任意大小的数据

**当前实现**：
- Map 阶段：顺序处理每个块（可以改为并行）
- Reduce 阶段：逐步合并（增量合并）

---

## 7. 实际应用案例

### 7.1 calculate_average_engagement_by_location()

```python
def calculate_average_engagement_by_location(filepath, chunk_size=5000):
    """
    Calculates the Average Engagement Rate (AER) grouped by IP location 
    for large datasets using chunked processing and partial aggregation merging.
    This demonstrates the project's scaling requirement.
    AER = (Total Reposts + Total Comments + Total Attitudes) / Total Posts
    """
```

**作用**：计算按 IP 位置分组的平均参与度（AER）

**业务背景**：
- 社交媒体数据分析
- 想知道不同地区的用户参与度
- 参与度 = (转发 + 评论 + 点赞) / 帖子数

**数据格式假设**：
```json
{
    "ip_location": "NYC",
    "reposts_count": 10,
    "comments_count": 5,
    "attitudes_count": 20,
    "text": "..."
}
```

---

### 7.2 步骤 1：初始化全局聚合容器

```python
# 1. initialize four global partial result containers
# Dictionaries to store merged partial aggregation results globally
partial_counts = {}          # 每个位置的帖子总数
partial_reposts_sums = {}    # 每个位置的转发总数
partial_comments_sums = {}   # 每个位置的评论总数
partial_attitudes_sums = {}  # 每个位置的点赞总数
```

**为什么需要四个容器**：
- 需要分别追踪四个指标
- 每个指标都需要分组聚合（按 `ip_location`）

**数据结构**：
```python
{
    "NYC": 100,  # NYC 位置的帖子数/转发数/评论数/点赞数
    "LA": 50,
    ...
}
```

---

### 7.3 步骤 2：分块处理（Map 阶段）

```python
# 2. process the file chunk by chunk
for chunk in load_json_chunks(filepath, chunk_size):
    coll = Collection(chunk)  # 将块转换为 Collection 对象
    
    # local aggregation calculations (Grouped by "ip_location")
    chunk_counts = coll.aggregate("ip_location", agg_count())
    chunk_reposts = coll.aggregate("ip_location", agg_sum("reposts_count"))
    chunk_comments = coll.aggregate("ip_location", agg_sum("comments_count"))
    chunk_attitudes = coll.aggregate("ip_location", agg_sum("attitudes_count"))
```

**详细流程**：

1. **加载块**：`load_json_chunks()` 生成器返回一个块的文档列表

2. **创建 Collection**：包装块数据，使用查询 API

3. **局部聚合**：对当前块进行四个聚合操作
   - `chunk_counts`：按位置计数（每个位置有多少帖子）
   - `chunk_reposts`：按位置求和转发数
   - `chunk_comments`：按位置求和评论数
   - `chunk_attitudes`：按位置求和点赞数

**局部结果示例**：
```python
# 假设当前块有 5000 条记录，处理后：
chunk_counts = {"NYC": 200, "LA": 150, "SF": 100}
chunk_reposts = {"NYC": 1000, "LA": 750, "SF": 500}
chunk_comments = {"NYC": 500, "LA": 375, "SF": 250}
chunk_attitudes = {"NYC": 2000, "LA": 1500, "SF": 1000}
```

---

### 7.4 步骤 3：合并局部结果（Reduce 阶段）

```python
# 3. merge Local Results
    
# Merge Counts (Total Posts)
for loc, count in chunk_counts.items():
    current_count = partial_counts.get(loc, 0)
    # use PartialAgg.merge_count to combine current global total with local chunk total
    partial_counts[loc] = PartialAgg.merge_count(current_count, count)
    
# Merge Reposts Sums
for loc, total in chunk_reposts.items():
    current_total = partial_reposts_sums.get(loc, 0)
    # Use PartialAgg.merge_sum for addition
    partial_reposts_sums[loc] = PartialAgg.merge_sum(current_total, total)
    
# Merge Comments Sums
for loc, total in chunk_comments.items():
    current_total = partial_comments_sums.get(loc, 0)
    partial_comments_sums[loc] = PartialAgg.merge_sum(current_total, total)
    
# Merge Attitudes (Likes) Sums
for loc, total in chunk_attitudes.items():
    current_total = partial_attitudes_sums.get(loc, 0)
    partial_attitudes_sums[loc] = PartialAgg.merge_sum(current_total, total)
```

**合并逻辑**：

对每个位置的每个指标：
1. 获取当前全局值（如果不存在，默认为 0）
2. 使用合并函数合并全局值和局部值
3. 更新全局值

**示例执行**：

```python
# 初始全局状态（处理第一个块后）
partial_counts = {"NYC": 200, "LA": 150}

# 第二个块的结果
chunk_counts = {"NYC": 300, "SF": 100}

# 合并过程：
# NYC: merge_count(200, 300) = 500
# LA: 保持 150（第二个块没有 LA）
# SF: merge_count(0, 100) = 100（新位置）

# 合并后的全局状态
partial_counts = {"NYC": 500, "LA": 150, "SF": 100}
```

**增量合并**：
- 每处理一个块，立即合并到全局结果
- 不需要存储所有块的局部结果
- 内存占用小

---

### 7.5 步骤 4：计算最终平均值

```python
# 4. Calculate Final Average Engagement Rate (Final Calculation)
final_results = {}
for loc in partial_counts:
    # Get all global sums
    total_interactions = (
        partial_reposts_sums.get(loc, 0) +
        partial_comments_sums.get(loc, 0) +
        partial_attitudes_sums.get(loc, 0)
    )
    total_posts = partial_counts[loc]
    
    # Calculate Average Engagement Rate, prevent division by zero
    avg_engagement_rate = total_interactions / total_posts if total_posts else 0
    
    final_results[loc] = {
        "Total_Posts": total_posts,
        "Avg_Engagement_Rate": avg_engagement_rate
    }

return final_results
```

**计算流程**：

1. **遍历所有位置**：使用 `partial_counts.keys()`

2. **计算总互动数**：
   ```python
   total_interactions = 转发数 + 评论数 + 点赞数
   ```

3. **计算平均参与度**：
   ```python
   AER = total_interactions / total_posts
   ```

4. **防止除零**：如果 `total_posts` 为 0，返回 0

**结果格式**：
```python
{
    "NYC": {
        "Total_Posts": 1000,
        "Avg_Engagement_Rate": 3.5  # 平均每条帖子有 3.5 次互动
    },
    "LA": {
        "Total_Posts": 500,
        "Avg_Engagement_Rate": 2.8
    }
}
```

**完整示例**：

假设处理完所有块后的全局状态：
```python
partial_counts = {"NYC": 1000, "LA": 500}
partial_reposts_sums = {"NYC": 2000, "LA": 800}
partial_comments_sums = {"NYC": 1000, "LA": 400}
partial_attitudes_sums = {"NYC": 500, "LA": 200}
```

计算 NYC：
```python
total_interactions = 2000 + 1000 + 500 = 3500
total_posts = 1000
AER = 3500 / 1000 = 3.5
```

计算 LA：
```python
total_interactions = 800 + 400 + 200 = 1400
total_posts = 500
AER = 1400 / 500 = 2.8
```

最终结果：
```python
{
    "NYC": {"Total_Posts": 1000, "Avg_Engagement_Rate": 3.5},
    "LA": {"Total_Posts": 500, "Avg_Engagement_Rate": 2.8}
}
```

---

### 7.6 算法总结

**完整流程图**：

```
文件（大文件）
    ↓
[分块加载] ← chunk_size=5000
    ↓
块 1 → [局部聚合] → 部分结果 1
块 2 → [局部聚合] → 部分结果 2
块 3 → [局部聚合] → 部分结果 3
...   ...           ...
    ↓
[合并部分结果] ← 使用 PartialAgg
    ↓
全局聚合结果
    ↓
[计算平均值]
    ↓
最终结果（按位置分组的 AER）
```

**时间复杂度**：
- Map 阶段：O(n)，n 是总记录数
- Reduce 阶段：O(m)，m 是唯一位置数（通常很小）
- 总复杂度：O(n)

**空间复杂度**：
- 块大小：O(chunk_size)
- 全局聚合：O(m)，m 是唯一位置数
- 总空间：O(chunk_size + m)

**可扩展性**：
- ✅ 支持任意大小的文件（通过调整 chunk_size）
- ✅ 可以并行化 Map 阶段（多线程/多进程）
- ✅ 可以分布式处理（多机器）

---

## 总结

这个项目实现了一个完整的**数据库查询系统**，从底层到应用层：

1. **底层解析**：自定义 JSON 解析器（Tokenizer + Parser）
2. **数据操作**：Collection 类提供丰富的查询 API
3. **聚合功能**：多种聚合函数支持
4. **大文件处理**：分块加载和部分聚合合并
5. **实际应用**：按位置计算平均参与度

**技术亮点**：
- 递归下降解析器
- 哈希连接算法
- Map-Reduce 模式
- 内存友好的分块处理
- 正确的聚合结果合并

**适用场景**：
- 社交媒体数据分析
- 日志数据分析
- 大规模 JSON 数据查询
- 数据仓库查询系统原型

